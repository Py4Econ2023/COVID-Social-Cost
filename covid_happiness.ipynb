{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# COVID-19 Infections and Happiness\n",
        "This is the notebook for the Python for Economics Project at the London  School of Economics analysing the effect of COVID-19 infections on happiness.\n"
      ],
      "metadata": {
        "id": "M_2dLRCIIqv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "As policy-making during an epidemic is all about making economic tradeoffs, one would like to quantify the gains and losses in the factors a government is is trading off between. The trade-offs to monetary factors and other classical economic factors are well documented, of course. However, the social costs of viral cases less so (among other forms of social costs involved in a pandemic). One may make an attempt to quantify the social costs of the number of cases of such a virus in your country by looking at the causal effect of COVID-19 cases on the average sentiment of how people express themselves online."
      ],
      "metadata": {
        "id": "c3t9AWywlLa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview Project\n",
        "In this project the main goal is to run a regression of the number of COVID-19 infections on the average sentiment of how people express themselves online. You will start by carrying out this analysis for the UK. A clear confounder here are government restrictions to curb the spread of the virus. You will control for this confounder in the regression alongside time-fixed effects that deal with the biases caused by new ways of measuring cases, changes in testing accuracy and availability, among other possible biases.\n",
        "</br></br>\n",
        "To be able to run this final regression, though, you will need to collect the data. This notebook will walk you through the steps associated with this and the final step of running the regression.\n"
      ],
      "metadata": {
        "id": "hpTdOHFalo5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents"
      ],
      "metadata": {
        "id": "loLc9eEEVSsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[COVID-19 Infections and Happiness](#scrollTo=M_2dLRCIIqv9)\n",
        "\n",
        ">>[Introduction](#scrollTo=c3t9AWywlLa_)\n",
        "\n",
        ">>[Overview Project](#scrollTo=hpTdOHFalo5C)\n",
        "\n",
        ">>[Table of Contents](#scrollTo=loLc9eEEVSsP)\n",
        "\n",
        ">>[Preparation](#scrollTo=5kqfEAq9S8KC)\n",
        "\n",
        ">>[Data Collection](#scrollTo=Xviu1_5NnsrF)\n",
        "\n",
        ">>>[Loading Datasets](#scrollTo=3gOgjpQpKGoe)\n",
        "\n",
        ">>>[Cleaning Datasets](#scrollTo=Xzmo0WIbKtrk)\n",
        "\n",
        ">>>>[Preparation](#scrollTo=Xzmo0WIbKtrk)\n",
        "\n",
        ">>>>[Stringency](#scrollTo=KB_HFVFsdT5H)\n",
        "\n",
        ">>>>[Cases](#scrollTo=6zlemmyxk2JT)\n",
        "\n",
        ">>>[Merging Dataframes](#scrollTo=8UXSCwxHg1Gi)\n",
        "\n",
        ">>>[Average Sentiment](#scrollTo=C_Wv1iKpmdo6)\n",
        "\n",
        ">>>>[Scraping Tweets](#scrollTo=UVZi_0ONKq5p)\n",
        "\n",
        ">>>>[Classifying Tweets](#scrollTo=zOiuZ0v8NSKA)\n",
        "\n",
        ">>[Running Regressions](#scrollTo=1wSD_8JrKslV)\n",
        "\n",
        ">>[Further Exercises](#scrollTo=HIx-MyVcN7Vh)\n",
        "\n",
        ">>[References](#scrollTo=GTQjBctvVLWv)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "nWLxb9_5S6Hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Preparation\n",
        "First, you will need to install a few libraries for this project. To install a library, write ``!pip install`` in a code block followed by ``name-library`` and the optional ``--quiet`` keyword to suppress the logs. For example, installing the package ``pandas`` can be done by running ``!pip install pandas --quiet`` in a code block.\n",
        "\n",
        "(note: between countries the definitions and methods of confirming cases differs. maybe look at percentual change in infections but then not the absolute size of infections. maybe ONS positive rates)."
      ],
      "metadata": {
        "id": "5kqfEAq9S8KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Install the following packages: pandas, datetime.\n",
        "!pip install pandas --quiet\n",
        "\n"
      ],
      "metadata": {
        "id": "zM3N6_a9gPMX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you have to import the packages you installed. Additionally, import the preinstalled package ``numpy`` as ``np``."
      ],
      "metadata": {
        "id": "bJkVLg9KJyNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Import the installed packages.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# One additional library necessary for CSV uploads is already given (no need to install this one, it is installed by default on Colabs).\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "LnuPMkjlJ_5L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collection\n",
        "We can now start collecting our data."
      ],
      "metadata": {
        "id": "Xviu1_5NnsrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Datasets\n",
        "The data we will use for this analysis will come from the John Hopkins University Center for Systems Science and Engineering, Our World in Data and, of course, Twitter. \n",
        "\n",
        "\n",
        "* The dataset on confirmed cases per country (including the UK) can be found and downloaded [here](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series).\n",
        "* The dataset on COVID-19 government restriction stringency can be found and downloaded [here](https://ourworldindata.org/covid-stringency-index).\n",
        "* We will get into the Tweets later.\n",
        "\n",
        "Once you have downloaded the datasets, you can upload one of them to to Colabs by running the comands below which will store a dataset as a Pandas dataframe (sort of like a spreadsheet). It is a good coding practice to wrap commands like this in a function. Do this and make the function output both datasets in a list. Then, call the function and assign the result to a variable ``dataframes``, storing the two dataframes in a list."
      ],
      "metadata": {
        "id": "3gOgjpQpKGoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This command will prompt you with an upload screen and store the uploaded files in a dictionary.\n",
        "# You can upload multiple files at once.\n",
        "uploaded = files.upload()\n",
        "\n",
        "# This command stores the filenames in a list.\n",
        "filenames = list(uploaded.keys())\n",
        "\n",
        "# This command selects the filename of the first file in the files you uploaded.\n",
        "filename = filenames[0]\n",
        "\n",
        "# This command stores a dataset in a variable as a Pandas dataframe.\n",
        "dataset = pd.read_csv('time_series_covid19_confirmed_global.csv')\n",
        "\n",
        "# TODO - Create and call the function."
      ],
      "metadata": {
        "id": "OaBrHMLZbnVY",
        "outputId": "87ccaa4b-84fa-4655-8f74-267ac5bd250c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f04f7996-d7b6-4e4c-b920-835eedf1bed1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f04f7996-d7b6-4e4c-b920-835eedf1bed1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving time_series_covid19_confirmed_global.csv to time_series_covid19_confirmed_global (7).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning Datasets\n",
        "#### Preparation\n",
        "First, it would be nice to have each of the datasets stored in a variable with a corresponding name. Below I show a trick to assign two variables at once. Use this trick to assign your datasets to the variables ``df_cases`` and ``df_stringency``."
      ],
      "metadata": {
        "id": "Xzmo0WIbKtrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cases = dataset\n",
        "\n",
        "# TODO - Replicate the trick with the variable names given."
      ],
      "metadata": {
        "id": "PzYAFEKgd8aF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stringency\n",
        "Let's start with the easiest dataset first. Inspect the structure of the dataset by printing the dataframe."
      ],
      "metadata": {
        "id": "KB_HFVFsdT5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - print the dataframe and inspect the structure."
      ],
      "metadata": {
        "id": "ZR-VoJcBKv6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly, there are lots of variables and countries of which we do not need the data. Therefore, we would like to drop the redundant entries. Do this by selecting only the date and stringency index values for just the United Kingdom. Overwrite ``df_stringency`` with this transformed dataframe. As a final nit-picky step, reset the index of the dataframe."
      ],
      "metadata": {
        "id": "cUWOFexDgayH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Overwrite the dataframe with the filtered version."
      ],
      "metadata": {
        "id": "xLjnB4JNhwBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We would like to have our data of suitable data types, so it is easiest to work with down the line. For example, we would like the values in our ``date`` column to be of the ``datetime`` data type. Also, we would like the values in our ``stringency_index`` column to be of the ``float`` data type. Check if this is the case and if not, convert the column values to the desired data type."
      ],
      "metadata": {
        "id": "qRnW1do6ioC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Check if the column values data types are correct and convert them if not."
      ],
      "metadata": {
        "id": "0KEX5HEsjbRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cases\n",
        "Now on to the harder dataset. Inspect the structure of the dataset by printing the dataframe."
      ],
      "metadata": {
        "id": "6zlemmyxk2JT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - print the dataframe and inspect the structure\n",
        "df_cases = df_cases.fillna('N/A')\n",
        "print(df_cases)\n",
        "print(df_cases.info())"
      ],
      "metadata": {
        "id": "0Yvhn1snloQx",
        "outputId": "bd6ef60c-4f79-4d5b-dcfd-2c1ec47f7370",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Province/State        Country/Region        Lat       Long  1/22/20  \\\n",
            "0              N/A           Afghanistan   33.93911  67.709953        0   \n",
            "1              N/A               Albania    41.1533    20.1683        0   \n",
            "2              N/A               Algeria    28.0339     1.6596        0   \n",
            "3              N/A               Andorra    42.5063     1.5218        0   \n",
            "4              N/A                Angola   -11.2027    17.8739        0   \n",
            "..             ...                   ...        ...        ...      ...   \n",
            "284            N/A    West Bank and Gaza    31.9522    35.2332        0   \n",
            "285            N/A  Winter Olympics 2022    39.9042   116.4074        0   \n",
            "286            N/A                 Yemen  15.552727  48.516388        0   \n",
            "287            N/A                Zambia -13.133897  27.849332        0   \n",
            "288            N/A              Zimbabwe -19.015438  29.154857        0   \n",
            "\n",
            "     1/23/20  1/24/20  1/25/20  1/26/20  1/27/20  ...  2/10/23  2/11/23  \\\n",
            "0          0        0        0        0        0  ...   208943   208971   \n",
            "1          0        0        0        0        0  ...   334229   334234   \n",
            "2          0        0        0        0        0  ...   271406   271409   \n",
            "3          0        0        0        0        0  ...    47860    47860   \n",
            "4          0        0        0        0        0  ...   105184   105184   \n",
            "..       ...      ...      ...      ...      ...  ...      ...      ...   \n",
            "284        0        0        0        0        0  ...   703228   703228   \n",
            "285        0        0        0        0        0  ...      535      535   \n",
            "286        0        0        0        0        0  ...    11945    11945   \n",
            "287        0        0        0        0        0  ...   342114   342288   \n",
            "288        0        0        0        0        0  ...   263083   263083   \n",
            "\n",
            "     2/12/23  2/13/23  2/14/23  2/15/23  2/16/23  2/17/23  2/18/23  2/19/23  \n",
            "0     208982   209011   209036   209056   209072   209083   209084   209107  \n",
            "1     334255   334255   334264   334264   334273   334291   334305   334314  \n",
            "2     271409   271409   271409   271421   271424   271424   271425   271425  \n",
            "3      47860    47860    47860    47860    47866    47866    47866    47866  \n",
            "4     105184   105184   105184   105184   105184   105184   105184   105184  \n",
            "..       ...      ...      ...      ...      ...      ...      ...      ...  \n",
            "284   703228   703228   703228   703228   703228   703228   703228   703228  \n",
            "285      535      535      535      535      535      535      535      535  \n",
            "286    11945    11945    11945    11945    11945    11945    11945    11945  \n",
            "287   342288   342317   342317   342317   342317   342317   342317   342317  \n",
            "288   263083   263083   263083   263642   263642   263642   263642   263642  \n",
            "\n",
            "[289 rows x 1129 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 289 entries, 0 to 288\n",
            "Columns: 1129 entries, Province/State to 2/19/23\n",
            "dtypes: int64(1125), object(4)\n",
            "memory usage: 2.5+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, there are a lot of countries we do not need the data of. Filter the dataframe to only contain records of the UK (be precise here) and overwrite the original dataframe with the filtered one."
      ],
      "metadata": {
        "id": "D9BK4l8Wl6-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Filter and overwrite the dataframe of cases.\n",
        "df_cases = df_cases.loc[(df_cases['Province/State'] == 'N/A') & (df_cases['Country/Region'] == 'United Kingdom')]\n",
        "print(df_cases)"
      ],
      "metadata": {
        "id": "pOa0oe0-oDd6",
        "outputId": "6de71596-729d-4c84-b887-77976a88980a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Province/State  Country/Region      Lat   Long  1/22/20  1/23/20  1/24/20  \\\n",
            "278            N/A  United Kingdom  55.3781 -3.436        0        0        0   \n",
            "\n",
            "     1/25/20  1/26/20  1/27/20  ...   2/10/23   2/11/23   2/12/23   2/13/23  \\\n",
            "278        0        0        0  ...  24315979  24315979  24315979  24315979   \n",
            "\n",
            "      2/14/23   2/15/23   2/16/23   2/17/23   2/18/23   2/19/23  \n",
            "278  24315979  24315979  24341611  24341611  24341611  24341611  \n",
            "\n",
            "[1 rows x 1129 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some might think we are done now with this dataset, but this dataset has a nasty characteristic. Namely, it is [*wide*](https://en.wikipedia.org/wiki/Wide_and_narrow_data), and quite *wide*, to say the least. Libraries written for Python and other programming languages hardly support this kind of data shape. Therefore, we want to change the shape of the data to the *narrow* format.\n",
        "</br></br>\n",
        "In essence, we would like one column for the date and one column for the confirmed cases. Thus, we need to put the column names in a new variable name called ``date`` and link the corresponding case numbers to the right row.\n",
        "</br></br>\n",
        "Convert the dataframe to a *narrow* format. After understanding the concepts by reading the Wikipedia page linked before, use Pandas' [``melt``](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) implementation to achieve this."
      ],
      "metadata": {
        "id": "oSMiKE20n8Va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Convert the dataframe from wide to narrow format.\n",
        "id_vars = df_cases.loc[:, ['Province/State']]\n",
        "value_vars = df_cases.loc[:, df_cases.columns != 'Province/State']\n",
        "df_cases = pd.melt(df_cases, id_vars == id_vars, value_vars == value_vars, var_name='Date', value_name='Cases', ignore_index=True)\n",
        "print(df_cases)"
      ],
      "metadata": {
        "id": "4289VUhkrPdh",
        "outputId": "28c2c549-338c-4178-ec68-bd025225aca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Province/State            Date           Cases\n",
            "0               N/A  Country/Region  United Kingdom\n",
            "1               N/A             Lat         55.3781\n",
            "2               N/A            Long          -3.436\n",
            "3               N/A         1/22/20               0\n",
            "4               N/A         1/23/20               0\n",
            "...             ...             ...             ...\n",
            "1123            N/A         2/15/23        24315979\n",
            "1124            N/A         2/16/23        24341611\n",
            "1125            N/A         2/17/23        24341611\n",
            "1126            N/A         2/18/23        24341611\n",
            "1127            N/A         2/19/23        24341611\n",
            "\n",
            "[1128 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_casesdropped = df_cases.drop(columns=['Province/State'])\n",
        "print(df_casesdropped)"
      ],
      "metadata": {
        "id": "QuL4O8pra2Ol",
        "outputId": "5bea0a05-030a-4293-d0ae-1796b3fac64b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                Date           Cases\n",
            "0     Country/Region  United Kingdom\n",
            "1                Lat         55.3781\n",
            "2               Long          -3.436\n",
            "3            1/22/20               0\n",
            "4            1/23/20               0\n",
            "...              ...             ...\n",
            "1123         2/15/23        24315979\n",
            "1124         2/16/23        24341611\n",
            "1125         2/17/23        24341611\n",
            "1126         2/18/23        24341611\n",
            "1127         2/19/23        24341611\n",
            "\n",
            "[1128 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_rowdropped = df_casesdropped.drop([1, 2])\n",
        "print(df_rowdropped)"
      ],
      "metadata": {
        "id": "ljB_5mTnbZKo",
        "outputId": "92b9740f-7730-4468-b12c-7cd4b0a890ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                Date           Cases\n",
            "0     Country/Region  United Kingdom\n",
            "3            1/22/20               0\n",
            "4            1/23/20               0\n",
            "5            1/24/20               0\n",
            "6            1/25/20               0\n",
            "...              ...             ...\n",
            "1123         2/15/23        24315979\n",
            "1124         2/16/23        24341611\n",
            "1125         2/17/23        24341611\n",
            "1126         2/18/23        24341611\n",
            "1127         2/19/23        24341611\n",
            "\n",
            "[1126 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cases = df_rowdropped.drop([0])\n",
        "print(df_cases)"
      ],
      "metadata": {
        "id": "ujbOiSeIcQkA",
        "outputId": "35932f8f-d333-4683-ef12-f473cada11a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date     Cases\n",
            "3     1/22/20         0\n",
            "4     1/23/20         0\n",
            "5     1/24/20         0\n",
            "6     1/25/20         0\n",
            "7     1/26/20         0\n",
            "...       ...       ...\n",
            "1123  2/15/23  24315979\n",
            "1124  2/16/23  24341611\n",
            "1125  2/17/23  24341611\n",
            "1126  2/18/23  24341611\n",
            "1127  2/19/23  24341611\n",
            "\n",
            "[1125 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we would like to convert the date column of data type ``string`` to the data type ``datetime``, because we want to link the time series datasets that we now have parsed to each other and make one big, complete dataset. This is not as easy as it was for the previous dataset, and you will probably find out why."
      ],
      "metadata": {
        "id": "spfwZAvLcQSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Convert the values of the date column to the datetime data type.\n",
        "df_cases['Date'] = pd.to_datetime(df_cases['Date'])\n",
        "print(df_cases)\n",
        "print(df_cases.dtypes)"
      ],
      "metadata": {
        "id": "fPc3C3lvdJTE",
        "outputId": "6cf2a59d-5db6-4203-94fe-b790406716d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Date     Cases\n",
            "3    2020-01-22         0\n",
            "4    2020-01-23         0\n",
            "5    2020-01-24         0\n",
            "6    2020-01-25         0\n",
            "7    2020-01-26         0\n",
            "...         ...       ...\n",
            "1123 2023-02-15  24315979\n",
            "1124 2023-02-16  24341611\n",
            "1125 2023-02-17  24341611\n",
            "1126 2023-02-18  24341611\n",
            "1127 2023-02-19  24341611\n",
            "\n",
            "[1125 rows x 2 columns]\n",
            "Date     datetime64[ns]\n",
            "Cases            object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merging Dataframes\n",
        "Now, we would like to merge the dataframes of the COVID-19 cases and COVID-19 policy stringency with eachother, so that for each date that is present in both dataframes we have one observation for the stringency and the number of cases. We will use [Pandas' implementation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) of a merge function."
      ],
      "metadata": {
        "id": "8UXSCwxHg1Gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Merge \"df_cases\" with \"df_stringency\" and save the result in a variable called \"df_cases_stringency\"."
      ],
      "metadata": {
        "id": "kgyVsh8zhYkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon inspecting the data, we can see that there are some missing observations for the stringency index, probably because the stringency data does not go as far in time as the cases dataset. To clean this up, we would like to drop these missing values."
      ],
      "metadata": {
        "id": "2v8hP7FCiN-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Drop the missing values in the dataset."
      ],
      "metadata": {
        "id": "OzT7B2VQi597"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After having edited data with code that takes a bit to run, you usually want to save your progress by downloading the dataset. (In more advanced projects, you would maybe use a database when using computationally expensive operations). Thus, download your dataset. You can use the previously installed ``files`` Colabs library for this. Download the dataset as ``cases_stringency.csv``. Make sure to exclude the index in the dataframe to CSV conversion step."
      ],
      "metadata": {
        "id": "u6QfU6-3i9kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Convert the dataframe to a CSV file and download it."
      ],
      "metadata": {
        "id": "XdaUbltCldrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average Sentiment\n",
        "Now, in the data collection part of this project we only have left the task of collecting data on the average sentiment of how people express themselves online."
      ],
      "metadata": {
        "id": "C_Wv1iKpmdo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scraping Tweets\n",
        "In this section, we will start scraping tweets from the UK in the same time period as variables ``stringency_index`` and ``cases`` are recorded in. In an academic setting, you might prefer to use an official Twitter API, but this can take a while to be admitted to. Additionally, few compromises are made by using an unofficial Twitter scraper.\n",
        "</br></br>\n",
        "If you have left off since everything before this code chunk and your Google Colabs runtime has restarted, you can optionally load the dataset you created in the previous parts with the code below."
      ],
      "metadata": {
        "id": "UVZi_0ONKq5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cases_stringency = upload_datasets()[0]"
      ],
      "metadata": {
        "id": "VBqdIVq2fM0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we install a library that allows us to easily scrape tweets from Twitter."
      ],
      "metadata": {
        "id": "IvQYiYAGfLum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snscrape --quiet"
      ],
      "metadata": {
        "id": "PgF_HqaSKsTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the scraping library."
      ],
      "metadata": {
        "id": "qEL1YIa3Vzx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import snscrape.modules.twitter as sntwitter"
      ],
      "metadata": {
        "id": "LjaDhGWXV8xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will have to define the date range we want to scrape data from before we start scraping tweets. A useful function for this is Pandas' [``date_range``](https://pandas.pydata.org/docs/reference/api/pandas.date_range.html). Define a date range that starts from the earliest date all the way to the last date in your dataframe ``df_cases_stringency``. Store this range of dates in a variable called ``date_range``."
      ],
      "metadata": {
        "id": "FBNc2EKonkt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Define the date range."
      ],
      "metadata": {
        "id": "oqIFugCXoSu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a list to store the tweets in."
      ],
      "metadata": {
        "id": "X7H18ZOUo5ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = []"
      ],
      "metadata": {
        "id": "fGRwhtEho7ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the number of tweets to be scraped per day. You can change this number to your liking. I would recommend to try running the code with this number first and possibly increasing it later when sure the code works so wasting computation time can be prevented."
      ],
      "metadata": {
        "id": "7k2LoAa6ovnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_per_day = 10"
      ],
      "metadata": {
        "id": "bUNAgrzTo-tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we want to scrape tweets published from the UK, we need to tell this to our scraper. As it so happens, Twitter uses geographic tags users can choose to attach to their tweets. (There are some problems of representativeness with this approach discussed [here](https://developer.twitter.com/en/docs/tutorials/advanced-filtering-for-geo-data) if you are interested.) The UK tag is ``6416b8512febefc9``. If needed while exploring the **optional** further exercises, you can find tags of other countries via the following Twitter API: ``f\"https://api.twitter.com/1.1/geo/reverse_geocode.json?lat={latitude}&lon={longitude}&granularity=country\"``. You would format the string based on your latitude and longitude variables before plugging the link in your browser or Python API module of choice. Documentation for this API can be found [here](https://developer.twitter.com/en/docs/twitter-api/v1/geo/places-near-location/api-reference/get-geo-reverse_geocode)."
      ],
      "metadata": {
        "id": "5MG-Qse2pfPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can start scraping. To get you started with the functionality of the ``snscrape`` module, I have written a simple piece of code that you can run to understand how this module can be used."
      ],
      "metadata": {
        "id": "pdPZ4bs7pDtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrating the working of the \"enumerate\" function.\n",
        "text_list = [\"This\", \"is\", \"how\", \"enumerate\", \"works.\"]\n",
        "for i, text in enumerate(text_list):\n",
        "  print(i, text)\n",
        "\n",
        "# Storing the place ID for the UK.\n",
        "place_id = \"6416b8512febefc9\"\n",
        "\n",
        "# Defining the search query for our Twitter scraper.\n",
        "# The keyword \"lang:en\" will filter for English tweets only.\n",
        "# The keywords \"since:date\" and \"until:date\" define the time range the tweet has to be from.\n",
        "# \"until\" is exclusive, meaning no tweets are scraped from \"2020-05-20\". \"since\" is inclusive.\n",
        "scraped_tweets = sntwitter.TwitterSearchScraper(f\"lang:en place:{place_id} since:2020-05-19 until:2020-05-20\").get_items()\n",
        "\n",
        "# This piece of code will print 5 tweets.\n",
        "# For each iteration in the loop, the scraper will scroll to the next tweet in the feed returned by Twitter.\n",
        "for i, tweet in enumerate(scraped_tweets):\n",
        "  print(tweet.rawContent)\n",
        "  print(tweet.date)\n",
        "  # We will only need the rawContent and date properties of the tweet.\n",
        "  # tweet.rawContent gives the text of the tweet (string)\n",
        "  # tweet.date gives the date and time of the tweet (datetime)\n",
        "  # For more properties, see line 60 and onwards of https://github.com/JustAnotherArchivist/snscrape/blob/master/snscrape/modules/twitter.py.\n",
        "\n",
        "  # Stopping the loop.\n",
        "  if i == 4:\n",
        "    break"
      ],
      "metadata": {
        "id": "1YAfwwvwnt3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you hopefully understand how this module works, I want you to write a function called ``scrape_time_range``.  This function will have to return a list of scraped tweets, containing the raw content and date for each tweet in the list.\n",
        "</br></br>\n",
        "This function should take four arguments:\n",
        "1. A list to append the scraped tweets to.\n",
        "2. The place ID.\n",
        "3. The date range.\n",
        "4. The number of tweets to be scraped per day.\n",
        "\n",
        "You want this function to iterate over the dates in the date range first, before defining the search query for that day and scraping the desired number of tweets. Notice that the dates stored in the previously created ``date_range`` are of the data type ``datetime``. They can be converted to strings by using the function [``strftime``](https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.strftime.html). You can format the desired output strings with the following keywords:\n",
        "* ``%Y`` which corresponds to YYYY.\n",
        "* ``%m`` which corresponds to mm.\n",
        "* ``%d`` which corresponds to dd.\n",
        "\n",
        "Make sure to take care of the hypens in these dates, too, when converting the date range, as your Twitter search query will be invalid without them. The same applies to the order of the year, month and date in the string.\n",
        "</br></br>\n",
        "**Hint:** wrap the output of ``date_range.strftime()`` in ``list()`` to convert the Numpy object to a Python list, which is more convenient in this instance."
      ],
      "metadata": {
        "id": "gaRxDrlytsTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# 1. Convert the date range to a list of date strings.\n",
        "# 2. Write the scraping function."
      ],
      "metadata": {
        "id": "KjWbM3VUvEod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call your scraping function.\n",
        "**Warning:** with 10 tweets a day this takes about 40 minutes to run and at a later stage the tweet classification task with the best model would take around 6 hours (but you can do this in batches of course)."
      ],
      "metadata": {
        "id": "Z9WZV9Rs2GDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Call it."
      ],
      "metadata": {
        "id": "K5y7RyOP4Tlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we would like to convert the list of tweets to a dataframe and a CSV file to save our progress. Call the dataframe ``df_tweets`` and the CSV file ``tweets.csv``."
      ],
      "metadata": {
        "id": "VTgGgrTe2AIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Convert the list of tweets to a dataframe and a CSV file."
      ],
      "metadata": {
        "id": "8sNpOp3V4Q1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classifying Tweets\n",
        "If you have left off before this chunk and your Colabs runtime has refreshed in the meantime, load the dataset below."
      ],
      "metadata": {
        "id": "zOiuZ0v8NSKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking the first index of the list of uploaded datasets, as you only upload one.\n",
        "df_tweets = upload_datasets()[0]"
      ],
      "metadata": {
        "id": "Ter9usq7NUal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage, we need to define a function that cleans the tweets. Namely, users and tweets mentioned in tweets might confuse the classification model that we will use at a later stage. This is possible if usernames and links have words in them that would refer to a certain sentiment but are not used for that purpose in natural text. Thus, we need to neutralise these words in the tweets. Create a function that converts all users (in the form of ``@username``) to \"``@user``\" and all links (in the form of ``https://`` to \"``https``\". Call it ``neutralise_mentions_links`` and make it so that it takes one argument called ``text``.\n",
        "</br></br>\n",
        "Use the ``.split()`` function of strings in Python. Mentions start with \"@\", links with \"https://\". "
      ],
      "metadata": {
        "id": "xtfCW3fQ5Tne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Write a function that removes mentions fom and shortens links in a piece of text."
      ],
      "metadata": {
        "id": "MO6uaFVH5lqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the function to all the tweets in the dataframe."
      ],
      "metadata": {
        "id": "K0QIey716y7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Apply the function to all tweets in the dataframe."
      ],
      "metadata": {
        "id": "6EmXTEYM8D_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we would like to classify the sentiment of the tweets in our dataframe. We task an external library with this exercise. The library we will use is ``happytransformer``. First, we install the library."
      ],
      "metadata": {
        "id": "o26msxfu9cX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install happytransformer --quiet"
      ],
      "metadata": {
        "id": "8eacejSm90vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, we import the text classification functionality from the library we installed."
      ],
      "metadata": {
        "id": "WkSvc5mO97xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from happytransformer import HappyTextClassification"
      ],
      "metadata": {
        "id": "vQ_7Pgwb9_0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Third, we load the AI model that has been trained on a large dataset of tweets with sentiment labels. We will use this for the analysis. This type of model is called a transformer model which you can read more on [here](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)."
      ],
      "metadata": {
        "id": "RXZMZTg8bJod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "happy_tc = HappyTextClassification(model_type=\"BERT\",  model_name=\"cardiffnlp/twitter-roberta-base-sentiment\", num_labels=3)"
      ],
      "metadata": {
        "id": "hKzPNn7XbTqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a demonstration of how the model can be used. Now write a function called ``classify_sentiment`` that takes in one argument of ``text`` and outputs the label in numeric form. \n",
        "</br></br>\n",
        "It is important for you to know that the label that the NLP model outputs is one of:\n",
        "* ``LABEL_0``, which corresponds to negative or the numeric form of -1.\n",
        "* ``LABEL_1``, which corresponds to neutral or the numeric form of 0.\n",
        "* ``LABEL_2``, which corresponds to positive or the numeric form of 1.\n",
        "\n",
        "The model outputs one score for each label and returns the label and score corresponding to the label with the highest score."
      ],
      "metadata": {
        "id": "LA8yizwwcU3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = happy_tc.classify_text(\"I think the Python for Economics week is a great initiative.\")\n",
        "print(result.label, result.score)\n",
        "\n",
        "# TODO - Write a function that outputs the label in numeric form."
      ],
      "metadata": {
        "id": "k8pjKAeAdJCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the ``sentiment_classifier`` function to the tweets and store the returned labels in a new column called ``sentiment``. **Warning:** doing this can be time intensive. This notebook was tested with 10 tweets per day and it took 6 hours to classify all the tweets scraped over the time range. Try doing this in chunks and downloading the results if you can't run the notebook for 6 hours straight."
      ],
      "metadata": {
        "id": "jHf31Zn_-A6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Apply the sentiment classifier function to the tweets."
      ],
      "metadata": {
        "id": "IKYTWYFH-UP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we want to calculate the average sentiment for each day. We can drop the column of tweets before we transform the dataframe. Store this new dataframe in a variable called ``df_sentiment``."
      ],
      "metadata": {
        "id": "AS7tT4sv-gjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Drop the column of tweets and transform the dataframe."
      ],
      "metadata": {
        "id": "R2A3TByi-wwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now successfully generated all of our data necessary for the analysis. One last thing to do is to merge the previously merged datasets with our final dataset of average sentiment scores to create the dataframe ``df_covid_happiness``. Download the dataset of the previously merged datasets with the code below if necessary."
      ],
      "metadata": {
        "id": "XGIUa7Jv-zfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cases_stringency = upload_datasets()[0]\n",
        "\n",
        "# TODO - Merge the stringency and cases dataset with the sentiment dataset."
      ],
      "metadata": {
        "id": "uxtkaQbI_Q2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we save the generated dataset."
      ],
      "metadata": {
        "id": "zJkAZLOs_wX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"covid_happiness.csv\"\n",
        "df_covid_happiness.to_csv(filename, index=False)\n",
        "files.download(filename)"
      ],
      "metadata": {
        "id": "z03uKSyn_yWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Regressions\n",
        "In this section you will have to run the following regression and report the results:\n",
        "$average\\_sentiment_t = \\beta positive\\_cases_t + \\gamma stringency_t + \\eta_t + \\varepsilon_t$\n",
        "</br></br>\n",
        "Before running this regression, think of the interpretation of the coefficient $\\beta$ if you run this regression. Would you want to rescale the corresponding variable $positive\\_cases$ with some proportion to improve the interpretability of this regression?\n",
        "</br></br>\n",
        "When interpreting the regression results you should make sure you understand the definitions of the variables used in the regression. For example, the number of confirmed cases for our purposes is actually the 7-day rolling average.\n",
        "</br></br>\n",
        "First we load our dataset if not loaded yet."
      ],
      "metadata": {
        "id": "1wSD_8JrKslV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_covid_happiness = upload_datasets()[0]"
      ],
      "metadata": {
        "id": "2qOTTUTGK3Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight the number of cases by some constant."
      ],
      "metadata": {
        "id": "3XtPB1Z1vGXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Weight the variable to improve the interpretability of the coefficient."
      ],
      "metadata": {
        "id": "nmWsxA_3vIp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now install the required packages for running regressions and generating the corresponding regression tables."
      ],
      "metadata": {
        "id": "W4dlwNVFAUP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install linearmodels --quiet"
      ],
      "metadata": {
        "id": "28Sx08QQAbhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then import the installed libraries."
      ],
      "metadata": {
        "id": "WO4OvB2DAer0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from linearmodels.panel import PanelOLS"
      ],
      "metadata": {
        "id": "rRA_fkvfAi30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we want to use month fixed effects in our regression. We will need to create a variable of month first in order to take this up in our final regression. Create a column that takes a different index for each month-year pair and wrap this in the function ``pd.Categorical()``."
      ],
      "metadata": {
        "id": "RARD-xurvTH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Create a column that takes a different index for each month-year pair."
      ],
      "metadata": {
        "id": "dwdYsRynzjoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, save and download the time series dataframe as ``covid_happiness_timeseries.csv``."
      ],
      "metadata": {
        "id": "bcAssm96mVSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Save and download the dataframe."
      ],
      "metadata": {
        "id": "LbrFE92Zz0cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifying the model. This is not an exercise because I am of the opinion that one should not learn to do their econometrics in Python and that the time spent searching the code to do this can be seen as suboptimally spent. Namely, documentation on econometric methods in Stata is arguably better and more intuitive to use for people with a background in economics."
      ],
      "metadata": {
        "id": "GDO8ivlwxRLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the date to the index as is required by the package of use.\n",
        "# Also, placing the index of dates in the first column.\n",
        "df_covid_happiness = df_covid_happiness.set_index(\"date\", append=True)\n",
        "df_covid_happiness.index = df_covid_happiness.index.swaplevel(0, 1)\n",
        "\n",
        "# Specifying the model.\n",
        "regression_model = PanelOLS(dependent=df_covid_happiness['sentiment'],\n",
        "                            exog=df_covid_happiness[[\"cases\", \"stringency_index\"]],\n",
        "                            entity_effects=False,\n",
        "                            time_effects=False,\n",
        "                            other_effects=df_covid_happiness['month'])"
      ],
      "metadata": {
        "id": "1I21FhoZxSU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the regression."
      ],
      "metadata": {
        "id": "nQuBPKQ2SUf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regression_results_summary = regression_model.fit(cov_type='clustered', cluster_entity=True).summary"
      ],
      "metadata": {
        "id": "_O1EGvq8SVz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a regression table with the results."
      ],
      "metadata": {
        "id": "5Y3O-SHaSWKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.latex.repr = True\n",
        "print(regression_results_summary)\n",
        "print(regression_results_summary.as_latex())"
      ],
      "metadata": {
        "id": "xKiQ_DjfSZE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Storing and downloading the regression table in ``LaTeX`` format."
      ],
      "metadata": {
        "id": "eZxVWkwjSalu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regression_table = open(\"regression_table.tex\", \"w\")\n",
        "regression_table = print(regression_results_summary.as_latex(), file=regression_table)\n",
        "files.download(\"regression_table.tex\")"
      ],
      "metadata": {
        "id": "ZwkHoFHxxk5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Exercises\n",
        "1. One option is to expand this analysis to different countries. Here, it is important to realise that comparing the coefficients of different countries is not justified. Namely, different countries may confirm cases in different ways. Looking at proportional increases in the number of cases will remove this problem, but will disregard the base level of new cases in the country which of course influences the magnitude of the effect on the average sentiment for a given proportional increase in the number of cases.\n",
        "2. Data visualisation: plot the comovement of the variables of interest over time or something else you are interested in seeing that can give a new insight into the problem.\n",
        "3. There may be other confounders present in the regression that I can't think of right now. If you can think of any, download the data for these, clean that data and create a new variable to run the regression with again.\n",
        "4. Scrape tweets from random time intervals to reduce bias induced by Twitter's feed selection methods. Documentation available for some of the keywords necessary in the Twitter search query to do this can be found [here](https://github.com/igorbrigadir/twitter-advanced-search).\n",
        "5. Filtering out spam tweets. You can approach this Natural Language Processing (NLP) problem in various ways, from as advanced as using AI classification methods as looking for duplicated tweets in your list of scraped tweets. You can always combine methods like these, of course.\n",
        "6. Improving the tweet cleaning function.\n",
        "7. Running the regression with different model specifications of how the confounder affects the outcome variable and the dependent variable. Namely, it may be the case that the start of heavy restrictions is not so bad yet, but that people get tired of it the longer these heavy restrictions are in place. You would need to transform the restriction variable to carry out the regression with this different definition of the control variable."
      ],
      "metadata": {
        "id": "HIx-MyVcN7Vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## References\n",
        "* Edouard Mathieu, Hannah Ritchie, Lucas Rodés-Guirao, Cameron Appel, Charlie Giattino, Joe Hasell, Bobbie Macdonald, Saloni Dattani, Diana Beltekian, Esteban Ortiz-Ospina and Max Roser (2020) - \"Coronavirus Pandemic (COVID-19)\". Published online at OurWorldInData.org. Retrieved from: 'https://ourworldindata.org/coronavirus' [Online Resource].\n",
        "* Ensheng Dong, Hongru Du, Lauren Gardner, An interactive web-based dashboard to track COVID-19 in real time, The Lancet Infectious Diseases, Volume 20, Issue 5, 2020, Pages 533-534, ISSN 1473-3099, https://doi.org/10.1016/S1473-3099(20)30120-1. (https://www.sciencedirect.com/science/article/pii/S1473309920301201).\n",
        "* JustAnotherArchivist, snscrape, (2023), GitHub repository, https://github.com/JustAnotherArchivist/snscrape.\n",
        "* igorbrigadir, Twitter Advanced Search, (2023), GitHub repository, https://github.com/igorbrigadir/twitter-advanced-search.\n",
        "* Wide and Narrow Data, Wikipedia, (12 Feb 2023), https://en.wikipedia.org/wiki/Wide_and_narrow_data.\n",
        "* Advanced Filtering for Geo Data, (2023), https://developer.twitter.com/en/docs/tutorials/advanced-filtering-for-geo-data.\n",
        "* Get Places Near a Location, (2023), https://developer.twitter.com/en/docs/twitter-api/v1/geo/places-near-location/api-reference/get-geo-reverse_geocode.\n",
        "\n"
      ],
      "metadata": {
        "id": "GTQjBctvVLWv"
      }
    }
  ]
}